{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0735d9e3-9f33-485c-b123-9d4fd89578eb",
   "metadata": {},
   "source": [
    "## 1. What is PyTorch?\n",
    "\n",
    "**Answer:**  \n",
    "PyTorch is an **open-source deep learning framework** developed by **Facebookâ€™s AI Research Lab (FAIR)**.  \n",
    "It provides a **flexible and efficient platform** for building, training, and deploying machine learning and deep learning models â€” especially suited for research because of its **imperative (Pythonic) programming style**.\n",
    "\n",
    "### ðŸ”‘ Key Features\n",
    "- **Dynamic computation graph (eager execution)** â€” graphs are built on-the-fly as operations run.  \n",
    "- **Strong GPU acceleration** via CUDA.  \n",
    "- **Autograd engine** for automatic differentiation.  \n",
    "- **`torch.nn` and `torch.optim`** modules for easy model building and optimization.  \n",
    "- **Seamless integration** with Python libraries like NumPy and SciPy.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Explain PyTorchâ€™s Dynamic Computation Graph and How It Differs from TensorFlowâ€™s Static Graph\n",
    "\n",
    "### ðŸ”¹ Dynamic Graph (PyTorch)\n",
    "- PyTorch builds the computation graph **dynamically during runtime**.  \n",
    "- Each operation (e.g., `a = x + y`) immediately creates a node in the graph **and executes it**.  \n",
    "- You can use **normal Python control flow** (loops, if-statements) inside models.  \n",
    "- The graph is **rebuilt each forward pass**, making it highly flexible and debuggable.  \n",
    "\n",
    "### ðŸ”¹ Static Graph (TensorFlow 1.x)\n",
    "- TensorFlow 1.x builds a **static graph before execution**.  \n",
    "- You first **define the full computation graph symbolically**, then call `session.run()` to execute it.  \n",
    "- Itâ€™s **more optimized for deployment**, but **less flexible for experimentation**.  \n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Summary Table**\n",
    "\n",
    "| Framework | Graph Type | When Built | Debugging | Flexibility |\n",
    "|------------|-------------|------------|------------|--------------|\n",
    "| **PyTorch** | Dynamic | At runtime | Easy (uses Python debugger) | Very high |\n",
    "| **TensorFlow 1.x** | Static | Before execution | Harder | Lower |\n",
    "| **TensorFlow 2.x (Eager Mode)** | Dynamic | Runtime | Easier | Similar to PyTorch |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eea8f9-4477-4967-95a8-411c17896ae8",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96977d-772c-4825-8c02-35e250ec8bd2",
   "metadata": {},
   "source": [
    "âœ… Example of placement:\n",
    "\n",
    "| Expression       | New shape | Description                          |\n",
    "|------------------|------------|--------------------------------------|\n",
    "| `X[None, :]`     | `(1, 2, 2)` | adds a new axis at the **front**     |\n",
    "| `X[:, None]`     | `(2, 1, 2)` | adds a new axis in the **middle**    |\n",
    "| `X[:, :, None]`  | `(2, 2, 1)` | adds a new axis at the **end**       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb6e3c1-7205-46a5-ad1e-12b65b003462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans(X, k=3, max_iters=100):\n",
    "    # Randomly choose k points as initial centroids\n",
    "    np.random.seed(42)\n",
    "    centroids = X[np.random.choice(len(X), k, replace=False)]\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Step 1: Assign points to nearest centroid\n",
    "        ## Broadcast [N*d] and [k*d] ==> [N*k*d] - [N*k*d]\n",
    "        distances = np.linalg.norm(X[:, None] - centroids[None, :], axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Step 2: Recompute centroids as mean of assigned points\n",
    "        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "\n",
    "        # Stop if centroids do not change\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916803c-6783-4b85-b8b9-715ec0620d30",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d40ef-7b0f-4b45-ae79-f61290122011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_linear_regression(X, y, lr, epochs, tol):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "\n",
    "    n,d = X.shape\n",
    "    Xb = np.c_[np.ones(n), X]\n",
    "    w = np.zeros(d+1)\n",
    "    history = [] \n",
    "\n",
    "    for _ in range(epochs):\n",
    "        y_hat = Xb@w\n",
    "        err = y_hat - y\n",
    "        loss = (err @ err) / (2 * n)\n",
    "\n",
    "        grad = (Xb.T @ err)/ n\n",
    "        w_new = w - lr * grad\n",
    "        if np.linalg.norm(w_new - w) < tol:\n",
    "            w = w_new\n",
    "            break\n",
    "        w = w_new\n",
    "    return w, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0cf4f8-ee1f-4aec-8a53-e618c468d8e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgd_linear_regression_torch\u001b[39m(X, y, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)   \u001b[38;5;66;03m# (n, d)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gd_linear_regression_torch(X, y, lr=1e-2, epochs=1000, tol=1e-6, device=\"cpu\"):\n",
    "    X = torch.as_tensor(X, dtype=torch.float32, device=device)   # (n, d)\n",
    "    y = torch.as_tensor(y, dtype=torch.float32, device=device).view(-1, 1)  # (n, 1)\n",
    "\n",
    "    n, d = X.shape\n",
    "    W = torch.zeros(d, 1, device=device, requires_grad=True)     # weights\n",
    "    b = torch.zeros(1, device=device, requires_grad=True)        # bias\n",
    "\n",
    "    history = []\n",
    "    for _ in range(epochs):\n",
    "        y_hat = X @ W + b                                        # (n,1)\n",
    "        loss = ((y_hat - y).pow(2).mean()) / 2                   # MSE/2\n",
    "        history.append(loss.item())\n",
    "\n",
    "        loss.backward()                                          # compute grads\n",
    "        with torch.no_grad():                                    # update params\n",
    "            W -= lr * W.grad\n",
    "            b -= lr * b.grad\n",
    "            # simple convergence check on parameter delta\n",
    "            if (W.grad.norm() * lr + b.grad.abs().sum() * lr) < tol:\n",
    "                pass\n",
    "            W.grad.zero_(); b.grad.zero_()                       # clear grads\n",
    "    return W.detach().cpu().numpy().ravel(), b.item(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3111c8-b544-4fae-8eca-3ec205a39ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearReg(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d, 1)  # includes weight and bias\n",
    "    def forward(self, X):\n",
    "        return self.fc(X)\n",
    "\n",
    "def train_lr(X, y, lr=1e-2, epochs=1000, device=\"cpu\"):\n",
    "    X = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "    y = torch.as_tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "    model = LinearReg(X.shape[1]).to(device)\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")                     # PyTorch's MSE\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    history = []\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()                                    # clear old grads\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, y) / 2                           # (optional) /2\n",
    "        history.append(loss.item())\n",
    "        loss.backward()                                          # autograd\n",
    "        optimizer.step()                                         # GD update\n",
    "    w = model.fc.weight.detach().cpu().numpy().ravel()\n",
    "    b = model.fc.bias.detach().cpu().item()\n",
    "    return w, b, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66854148-de61-4fd8-9d79-f749d5de57fa",
   "metadata": {},
   "source": [
    "## Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d97e29f-5040-436c-8ea6-ebdc6a643835",
   "metadata": {},
   "source": [
    "1) **Linear regression (the simplest backprop)**\n",
    "\n",
    "**Model**  \n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "**Loss (MSE)**  \n",
    "$$\n",
    "L = \\frac{1}{N}\\sum_{i=1}^{N}\\lVert \\hat{y}_i - y_i \\rVert^2\n",
    "$$\n",
    "\n",
    "**Gradients**  \n",
    "$$\n",
    "\\nabla_{\\hat{y}} L = \\frac{2}{N}\\,(\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_W L = X^\\top \\,\\nabla_{\\hat{y}} L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b L = \\sum_{i=1}^{N} \\left(\\nabla_{\\hat{y}} L\\right)_i\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41049bda-81eb-4c1f-973c-7a950d53b52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [ 2.00512262 -3.00671982  1.4890194 ] b: [0.69656805]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "N, D, O = 128, 3, 1          # samples, input dim, output dim\n",
    "X = np.random.randn(N, D)\n",
    "true_W = np.array([[2.0],[ -3.0],[1.5]])\n",
    "true_b = np.array([0.7])\n",
    "y = X @ true_W + true_b + 0.1*np.random.randn(N, O)\n",
    "\n",
    "W = np.random.randn(D, O) * 0.01\n",
    "b = np.zeros(O)\n",
    "lr = 1e-2\n",
    "\n",
    "for _ in range(1000):\n",
    "    # forward\n",
    "    y_hat = X @ W + b         # (N,O)\n",
    "    loss = np.mean((y_hat - y)**2)\n",
    "\n",
    "    # backward\n",
    "    dYhat = (2.0 / N) * (y_hat - y)     # (N,O)\n",
    "    dW = X.T @ dYhat                     # (D,O)\n",
    "    db = dYhat.sum(axis=0)               # (O,)\n",
    "\n",
    "    # update\n",
    "    W -= lr * dW\n",
    "    b -= lr * db\n",
    "\n",
    "print(\"W:\", W.ravel(), \"b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8e906-2a56-413d-91c8-8e5b7f23cc93",
   "metadata": {},
   "source": [
    "## 2) Two-layer NN (ReLU -> Linear -> Softmax) â€” Backprop\n",
    "\n",
    "**Architecture**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_1 &= X W_1 + \\mathbf{1} b_1^{\\top} \\\\\n",
    "h  &= \\mathrm{ReLU}(Z_1) \\\\\n",
    "\\text{scores} &= h W_2 + \\mathbf{1} b_2^{\\top} \\\\\n",
    "\\text{probs}_{i,j} &= \\frac{\\exp(\\text{scores}_{i,j})}{\\sum_{c=1}^{C} \\exp(\\text{scores}_{i,c})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Cross-entropy loss (labels as class indices \\(y_i\\))**\n",
    "$$\n",
    "L \\;=\\; -\\frac{1}{N}\\sum_{i=1}^{N}\\log\\!\\big(\\text{probs}_{i,\\,y_i}\\big)\n",
    "$$\n",
    "\n",
    "**Softmax + CE gradient (vectorized)**\n",
    "Let $Y \\in \\mathbb{R}^{N \\times C}$ be one-hot labels. Then\n",
    "$$\n",
    "G \\;=\\; \\frac{1}{N}\\,\\big(\\text{probs} - Y\\big)\n",
    "$$\n",
    "\n",
    "**Top linear layer**\n",
    "$$\n",
    "\\nabla_{W_2} L \\;=\\; h^{\\top} G, \n",
    "\\qquad\n",
    "\\nabla_{b_2} L \\;=\\; \\sum_{i=1}^{N} G_{i,\\cdot},\n",
    "\\qquad\n",
    "\\nabla_{h} L \\;=\\; G W_2^{\\top}\n",
    "$$\n",
    "\n",
    "**ReLU backward**\n",
    "Let $Z_1 = X W_1 + \\mathbf{1}\\, b_1^{\\top}$. Then\n",
    "$$\n",
    "\\nabla_{h}^{(\\text{after})} \\;=\\; \\nabla_{h}^{(\\text{before})} \\;\\odot\\; \\mathbb{I}[\\,Z_1 > 0\\,]\n",
    "$$\n",
    "\n",
    "**Bottom linear layer**\n",
    "$$\n",
    "\\nabla_{W_1} L \\;=\\; X^{\\top}\\,\\nabla_{h}^{(\\text{after})},\n",
    "\\qquad\n",
    "\\nabla_{b_1} L \\;=\\; \\sum_{i=1}^{N} \\nabla_{h,i}^{(\\text{after})}\n",
    "$$\n",
    "\n",
    "**Shape reference**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&X\\in\\mathbb{R}^{N\\times D},\\quad W_1\\in\\mathbb{R}^{D\\times H},\\quad b_1\\in\\mathbb{R}^{H} \\\\\n",
    "&h\\in\\mathbb{R}^{N\\times H},\\quad W_2\\in\\mathbb{R}^{H\\times C},\\quad b_2\\in\\mathbb{R}^{C} \\\\\n",
    "&\\text{scores},\\,\\text{probs},\\,Y,\\,G\\in\\mathbb{R}^{N\\times C}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3c367d-553f-4406-b773-a2ea5f1c8d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.9784, acc=0.490\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "N, D, H, C = 200, 4, 16, 3        # samples, input dim, hidden, classes\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(0, C, size=N)\n",
    "\n",
    "W1 = 0.01*np.random.randn(D, H); b1 = np.zeros(H)\n",
    "W2 = 0.01*np.random.randn(H, C); b2 = np.zeros(C)\n",
    "\n",
    "lr = 1e-0\n",
    "\n",
    "for _ in range(1000):\n",
    "    # ----- forward -----\n",
    "    z1 = X @ W1 + b1           # (N,H)\n",
    "    h = np.maximum(0, z1)      # ReLU (N,H)\n",
    "    scores = h @ W2 + b2       # (N,C)\n",
    "\n",
    "    # softmax (stable)\n",
    "    scores_shift = scores - scores.max(axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(scores_shift)\n",
    "    probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)   # (N,C)\n",
    "\n",
    "    # cross-entropy loss\n",
    "    correct_logprobs = -np.log(probs[np.arange(N), y] + 1e-12)\n",
    "    ## Binary case\n",
    "    # loss = -np.mean(y * np.log(probs + 1e-12) + (1 - y) * np.log(1 - probs + 1e-12))\n",
    "    loss = correct_logprobs.mean()\n",
    "\n",
    "    # ----- backward -----\n",
    "    dscores = probs.copy()                    # (N,C)\n",
    "    dscores[np.arange(N), y] -= 1\n",
    "    dscores /= N\n",
    "\n",
    "    dW2 = h.T @ dscores                      # (H,C)\n",
    "    db2 = dscores.sum(axis=0)                # (C,)\n",
    "\n",
    "    dh = dscores @ W2.T                      # (N,H)\n",
    "    dh[z1 <= 0] = 0                          # ReLU backprop\n",
    "\n",
    "    dW1 = X.T @ dh                           # (D,H)\n",
    "    db1 = dh.sum(axis=0)                     # (H,)\n",
    "\n",
    "    # ----- update -----\n",
    "    W1 -= lr * dW1; b1 -= lr * db1\n",
    "    W2 -= lr * dW2; b2 -= lr * db2\n",
    "\n",
    "# quick sanity check: accuracy\n",
    "scores = np.maximum(0, X @ W1 + b1) @ W2 + b2\n",
    "pred = scores.argmax(axis=1)\n",
    "acc = (pred == y).mean()\n",
    "print(f\"loss={loss:.4f}, acc={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1828653-ae6b-4ed4-9d13-9fca3db77173",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d38d7f-19dd-464e-9ec0-c2882051bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_heads: int):\n",
    "        ## Identical to super().__init__\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size must be divisible by the number of heads.\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values: torch.Tensor, keys: torch.Tensor, queries: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Split the embedding into self.num_heads different pieces\n",
    "        values = self.values(values).reshape(batch_size, value_len, self.num_heads, self.head_dim)\n",
    "        keys = self.keys(keys).reshape(batch_size, key_len, self.num_heads, self.head_dim)\n",
    "        queries = self.queries(queries).reshape(batch_size, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose for attention dot product: (batch_size, num_heads, seq_len, head_dim)\n",
    "        values = values.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        ## Before queries = [B, num_heads, query_len, head_dim]\n",
    "        ##        keys    = [B, num_heads, key_len, head_dim]\n",
    "        ## keys.transpose(-2, -1)) swaps the last 2 dimensions ==> (B, H, N, L)\n",
    "        energy = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        ## Mask usually has shape [batch_size, key_len]\n",
    "        ## Unsqueeze make it [B, 1, 1, K_len]\n",
    "        if mask is not None:\n",
    "            # Expand mask to match energy shape\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            mask = mask.expand(-1, self.num_heads, query_len, -1)\n",
    "            energy = energy.masked_fill(mask == 0, -float(\"Inf\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)  # (batch_size, num_heads, query_len, head_dim)\n",
    "        ## forces PyTorch to actually copy the tensor into a new block of memory laid out in row-major (C-style) order.\n",
    "        out = out.transpose(1, 2).contiguous()  # (batch_size, query_len, num_heads, head_dim)\n",
    "        out = out.reshape(batch_size, query_len, self.embed_size)  # Concatenate heads\n",
    "\n",
    "        out = self.fc_out(out)  # Apply the final linear layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3009596-2523-4ce1-84a3-e2b8ce750f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_heads: int):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size must be divisible by the number of heads.\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        # 5000 is the max relative distance\n",
    "        self.relative_position_embeddings = nn.Parameter(\n",
    "            torch.randn((2 * 5000 - 1, self.head_dim))\n",
    "        )\n",
    "\n",
    "    def forward(self, values: torch.Tensor, keys: torch.Tensor, queries: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        values = self.values(values).reshape(batch_size, value_len, self.num_heads, self.head_dim)\n",
    "        keys = self.keys(keys).reshape(batch_size, key_len, self.num_heads, self.head_dim)\n",
    "        queries = self.queries(queries).reshape(batch_size, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values = values.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        ## Trick to use broadcast\n",
    "        relative_positions = torch.arange(query_len).unsqueeze(1) - torch.arange(key_len).unsqueeze(0)\n",
    "        # Example:\n",
    "        # tensor([[2, 1, 0, 0],\n",
    "        #         [3, 2, 1, 0],\n",
    "        #         [4, 3, 2, 1],\n",
    "        #         [4, 4, 3, 2]])\n",
    "        clipped_positions = torch.clamp(relative_positions + 5000 - 1, min=0, max=2 * 5000 - 2)\n",
    "        ## EâˆˆR(2Lâˆ’1)Ã—dh look up table\n",
    "        relative_embeddings = self.relative_position_embeddings[clipped_positions]  # (query_len, key_len, head_dim)\n",
    "\n",
    "        # Scaled dot-product attention with relative positional embeddings\n",
    "        energy = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5\n",
    "        # for each position q, we extract k of d dimensional position verctor\n",
    "        # You give it a string equation that describes how indices line up, and PyTorch does the math automatically.\n",
    "        energy += torch.einsum('bhqd,qkd->bhqk', queries, relative_embeddings)  # Add relative positional embeddings\n",
    "\n",
    "        if mask is not None:\n",
    "            # Expand mask to match energy shape\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            mask = mask.expand(-1, self.num_heads, query_len, -1)\n",
    "            energy = energy.masked_fill(mask == 0, -float(\"Inf\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.reshape(batch_size, query_len, self.embed_size)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73289f2-670a-4ba2-b3c0-c216ca99ed48",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdb4cb9b-e0a2-4343-b3fd-77c3533ffcbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'â†’' (U+2192) (2483536979.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"hello ðŸ˜Š\" â†’ \"ðŸ˜Š\" becomes <unk>\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'â†’' (U+2192)\n"
     ]
    }
   ],
   "source": [
    "\"hello ðŸ˜Š\" â†’ \"ðŸ˜Š\" becomes <unk>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380ba47-577b-4612-a541-58269935e20b",
   "metadata": {},
   "source": [
    "The tokenizer only knows merge rules learned during training.\n",
    "If a new word contains a character sequence that never appeared during training,\n",
    "the tokenizer doesnâ€™t know how to split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0f1ed-5339-4918-b878-b08eda47385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, vocab_size: int, min_frequency: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer with the desired vocabulary size and minimum frequency for pair merges.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def train(self, corpus: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on the given corpus.\n",
    "        \"\"\"\n",
    "        # Build initial word frequency and split words into characters\n",
    "        word_freqs = defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                word = word + ' '  # Add a space at the end of each word to preserve word boundaries\n",
    "                word_freqs[word] += 1\n",
    "        ## word_freqs = {\"the \": 2, \"cat \": 1, \"dog \": 1}\n",
    "\n",
    "        # Initialize splits: each word is split into a list of characters\n",
    "        splits = {word: list(word) for word in word_freqs}\n",
    "\n",
    "        # splits = {\n",
    "        #   \"the \": [\"t\", \"h\", \"e\", \" \"],\n",
    "        #   \"cat \": [\"c\", \"a\", \"t\", \" \"],\n",
    "        #   \"dog \": [\"d\", \"o\", \"g\", \" \"]\n",
    "        # }\n",
    "\n",
    "        # Build vocabulary with single characters\n",
    "        alphabet = set()\n",
    "        for word in splits:\n",
    "            for char in splits[word]:\n",
    "                alphabet.add(char)\n",
    "\n",
    "        ## alphabet = {\"t\", \"h\", \"e\", \" \", \"c\", \"a\", \"d\", \"o\", \"g\"}\n",
    "\n",
    "        # Initialize vocabulary with single characters\n",
    "        self.vocab = {char: idx for idx, char in enumerate(alphabet)}\n",
    "\n",
    "        # self.vocab = {\n",
    "        #   \"t\": 0,\n",
    "        #   \"h\": 1,\n",
    "        #   \"e\": 2,\n",
    "        #   \" \": 3,\n",
    "        #   \"c\": 4,\n",
    "        #   \"a\": 5,\n",
    "        #   \"d\": 6,\n",
    "        #   \"o\": 7,\n",
    "        #   \"g\": 8\n",
    "        # }\n",
    "\n",
    "        # Continue merging pairs until the vocabulary reaches the desired size\n",
    "        # After merging \"t\" and \"h\", we have \"t\", \"h\" and \"th\"\n",
    "        # {\n",
    "        #   ('t', 'h'): 2,\n",
    "        #   ('h', 'e'): 2,\n",
    "        #   ('e', ' '): 2,\n",
    "        #   ('c', 'a'): 1,\n",
    "        #   ('a', 't'): 1,\n",
    "        #   ('t', ' '): 1,\n",
    "        #   ('d', 'o'): 1,\n",
    "        #   ('o', 'g'): 1,\n",
    "        #   ('g', ' '): 1\n",
    "        # }\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freqs = self._get_pair_frequencies(splits, word_freqs)\n",
    "\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "\n",
    "            # Get the most frequent pair of symbols\n",
    "            best_pair, best_freq = max(pair_freqs.items(), key=lambda x: x[1])\n",
    "\n",
    "            if best_freq < self.min_frequency:\n",
    "                break\n",
    "\n",
    "            # Merge the most frequent pair\n",
    "            splits = self._merge_pair(best_pair, splits)\n",
    "\n",
    "            # splits = {\n",
    "            #   \"the \": [\"th\", \"e\", \" \"],\n",
    "            #   \"cat \": [\"c\", \"a\", \"t\", \" \"],\n",
    "            #   \"dog \": [\"d\", \"o\", \"g\", \" \"]\n",
    "            # }\n",
    "            \n",
    "            # Add the new merge to the vocabulary and merges table\n",
    "            new_symbol = best_pair[0] + best_pair[1]\n",
    "            ## \"th\"\n",
    "            self.vocab[new_symbol] = len(self.vocab)\n",
    "            self.merges[best_pair] = new_symbol\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token ids.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "\n",
    "        for word in words:\n",
    "            word = word + ' '  # Add a space at the end of each word to preserve word boundaries\n",
    "            split_word = list(word)\n",
    "            for pair, merge in self.merges.items():\n",
    "                i = 0\n",
    "                while i < len(split_word) - 1:\n",
    "                    if split_word[i] == pair[0] and split_word[i + 1] == pair[1]:\n",
    "                        split_word = split_word[:i] + [merge] + split_word[i + 2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            # Convert merged word parts to token ids\n",
    "            for token in split_word:\n",
    "                tokens.append(self.vocab.get(token, -1))  # -1 for unknown tokens\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode the list of token ids back into text.\n",
    "        \"\"\"\n",
    "        ## tokens = [11, 4, 5, 0, 3]\n",
    "        ## [\"the \", \"c\", \"a\", \"t\", \" \"]\n",
    "        # Inverse the vocabulary dictionary to get symbol to token mapping\n",
    "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        decoded_text = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in inv_vocab:\n",
    "                decoded_text.append(inv_vocab[token])\n",
    "            else:\n",
    "                decoded_text.append('<UNK>')  # Handle unknown tokens\n",
    "\n",
    "        # Join the decoded text and strip the trailing spaces after joining\n",
    "        return ''.join(decoded_text).strip()\n",
    "\n",
    "    def _get_pair_frequencies(self, splits: dict, word_freqs: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Get the frequency of all symbol pairs in the current split words.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in word_freqs.items():\n",
    "            split = splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "    def _merge_pair(self, pair: Tuple[str, str], splits: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Merge the given pair in all the words.\n",
    "        \"\"\"\n",
    "        for word in splits:\n",
    "            split = splits[word]\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [pair[0] + pair[1]] + split[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[word] = split\n",
    "        return splits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
