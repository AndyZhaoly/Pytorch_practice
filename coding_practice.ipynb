{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0735d9e3-9f33-485c-b123-9d4fd89578eb",
   "metadata": {},
   "source": [
    "## 1. What is PyTorch?\n",
    "\n",
    "**Answer:**  \n",
    "PyTorch is an **open-source deep learning framework** developed by **Facebookâ€™s AI Research Lab (FAIR)**.  \n",
    "It provides a **flexible and efficient platform** for building, training, and deploying machine learning and deep learning models â€” especially suited for research because of its **imperative (Pythonic) programming style**.\n",
    "\n",
    "### ðŸ”‘ Key Features\n",
    "- **Dynamic computation graph (eager execution)** â€” graphs are built on-the-fly as operations run.  \n",
    "- **Strong GPU acceleration** via CUDA.  \n",
    "- **Autograd engine** for automatic differentiation.  \n",
    "- **`torch.nn` and `torch.optim`** modules for easy model building and optimization.  \n",
    "- **Seamless integration** with Python libraries like NumPy and SciPy.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Explain PyTorchâ€™s Dynamic Computation Graph and How It Differs from TensorFlowâ€™s Static Graph\n",
    "\n",
    "### ðŸ”¹ Dynamic Graph (PyTorch)\n",
    "- PyTorch builds the computation graph **dynamically during runtime**.  \n",
    "- Each operation (e.g., `a = x + y`) immediately creates a node in the graph **and executes it**.  \n",
    "- You can use **normal Python control flow** (loops, if-statements) inside models.  \n",
    "- The graph is **rebuilt each forward pass**, making it highly flexible and debuggable.  \n",
    "\n",
    "### ðŸ”¹ Static Graph (TensorFlow 1.x)\n",
    "- TensorFlow 1.x builds a **static graph before execution**.  \n",
    "- You first **define the full computation graph symbolically**, then call `session.run()` to execute it.  \n",
    "- Itâ€™s **more optimized for deployment**, but **less flexible for experimentation**.  \n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Summary Table**\n",
    "\n",
    "| Framework | Graph Type | When Built | Debugging | Flexibility |\n",
    "|------------|-------------|------------|------------|--------------|\n",
    "| **PyTorch** | Dynamic | At runtime | Easy (uses Python debugger) | Very high |\n",
    "| **TensorFlow 1.x** | Static | Before execution | Harder | Lower |\n",
    "| **TensorFlow 2.x (Eager Mode)** | Dynamic | Runtime | Easier | Similar to PyTorch |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eea8f9-4477-4967-95a8-411c17896ae8",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96977d-772c-4825-8c02-35e250ec8bd2",
   "metadata": {},
   "source": [
    "âœ… Example of placement:\n",
    "\n",
    "| Expression       | New shape | Description                          |\n",
    "|------------------|------------|--------------------------------------|\n",
    "| `X[None, :]`     | `(1, 2, 2)` | adds a new axis at the **front**     |\n",
    "| `X[:, None]`     | `(2, 1, 2)` | adds a new axis in the **middle**    |\n",
    "| `X[:, :, None]`  | `(2, 2, 1)` | adds a new axis at the **end**       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6e3c1-7205-46a5-ad1e-12b65b003462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans(X, k=3, max_iters=100):\n",
    "    # Randomly choose k points as initial centroids\n",
    "    np.random.seed(42)\n",
    "    centroids = X[np.random.choice(len(X), k, replace=False)]\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Step 1: Assign points to nearest centroid\n",
    "        ## Broadcast [N*d] and [k*d] ==> [N*k*d] - [N*k*d]\n",
    "        distances = np.linalg.norm(X[:, None] - centroids[None, :], axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Step 2: Recompute centroids as mean of assigned points\n",
    "        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "\n",
    "        # Stop if centroids do not change\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916803c-6783-4b85-b8b9-715ec0620d30",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d40ef-7b0f-4b45-ae79-f61290122011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_linear_regression(X, y, lr, epochs, tol):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "\n",
    "    n,d = X.shape\n",
    "    Xb = np.c_[np.ones(n), X]\n",
    "    w = np.zeros(d+1)\n",
    "    history = [] \n",
    "\n",
    "    for _ in range(epochs):\n",
    "        y_hat = Xb@w\n",
    "        err = y_hat - y\n",
    "        loss = (err @ err) / (2 * n)\n",
    "\n",
    "        grad = (Xb.T @ err)/ n\n",
    "        w_new = w - lr * grad\n",
    "        if np.linalg.norm(w_new - w) < tol:\n",
    "            w = w_new\n",
    "            break\n",
    "        w = w_new\n",
    "    return w, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0cf4f8-ee1f-4aec-8a53-e618c468d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gd_linear_regression_torch(X, y, lr=1e-2, epochs=1000, tol=1e-6, device=\"cpu\"):\n",
    "    X = torch.as_tensor(X, dtype=torch.float32, device=device)   # (n, d)\n",
    "    y = torch.as_tensor(y, dtype=torch.float32, device=device).view(-1, 1)  # (n, 1)\n",
    "\n",
    "    n, d = X.shape\n",
    "    W = torch.zeros(d, 1, device=device, requires_grad=True)     # weights\n",
    "    b = torch.zeros(1, device=device, requires_grad=True)        # bias\n",
    "\n",
    "    history = []\n",
    "    for _ in range(epochs):\n",
    "        y_hat = X @ W + b                                        # (n,1)\n",
    "        loss = ((y_hat - y).pow(2).mean()) / 2                   # MSE/2\n",
    "        history.append(loss.item())\n",
    "\n",
    "        loss.backward()                                          # compute grads\n",
    "        with torch.no_grad():                                    # update params\n",
    "            W -= lr * W.grad\n",
    "            b -= lr * b.grad\n",
    "            # simple convergence check on parameter delta\n",
    "            if (W.grad.norm() * lr + b.grad.abs().sum() * lr) < tol:\n",
    "                pass\n",
    "            W.grad.zero_(); b.grad.zero_()                       # clear grads\n",
    "    return W.detach().cpu().numpy().ravel(), b.item(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3111c8-b544-4fae-8eca-3ec205a39ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearReg(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d, 1)  # includes weight and bias\n",
    "    def forward(self, X):\n",
    "        return self.fc(X)\n",
    "\n",
    "def train_lr(X, y, lr=1e-2, epochs=1000, device=\"cpu\"):\n",
    "    X = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "    y = torch.as_tensor(y, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "    model = LinearReg(X.shape[1]).to(device)\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")                     # PyTorch's MSE\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    history = []\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()                                    # clear old grads\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, y) / 2                           # (optional) /2\n",
    "        history.append(loss.item())\n",
    "        loss.backward()                                          # autograd\n",
    "        optimizer.step()                                         # GD update\n",
    "    w = model.fc.weight.detach().cpu().numpy().ravel()\n",
    "    b = model.fc.bias.detach().cpu().item()\n",
    "    return w, b, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66854148-de61-4fd8-9d79-f749d5de57fa",
   "metadata": {},
   "source": [
    "## Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d97e29f-5040-436c-8ea6-ebdc6a643835",
   "metadata": {},
   "source": [
    "1) **Linear regression (the simplest backprop)**\n",
    "\n",
    "**Model**  \n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "**Loss (MSE)**  \n",
    "$$\n",
    "L = \\frac{1}{N}\\sum_{i=1}^{N}\\lVert \\hat{y}_i - y_i \\rVert^2\n",
    "$$\n",
    "\n",
    "**Gradients**  \n",
    "$$\n",
    "\\nabla_{\\hat{y}} L = \\frac{2}{N}\\,(\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_W L = X^\\top \\,\\nabla_{\\hat{y}} L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b L = \\sum_{i=1}^{N} \\left(\\nabla_{\\hat{y}} L\\right)_i\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41049bda-81eb-4c1f-973c-7a950d53b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "N, D, O = 128, 3, 1          # samples, input dim, output dim\n",
    "X = np.random.randn(N, D)\n",
    "true_W = np.array([[2.0],[ -3.0],[1.5]])\n",
    "true_b = np.array([0.7])\n",
    "y = X @ true_W + true_b + 0.1*np.random.randn(N, O)\n",
    "\n",
    "W = np.random.randn(D, O) * 0.01\n",
    "b = np.zeros(O)\n",
    "lr = 1e-2\n",
    "\n",
    "for _ in range(1000):\n",
    "    # forward\n",
    "    y_hat = X @ W + b         # (N,O)\n",
    "    loss = np.mean((y_hat - y)**2)\n",
    "\n",
    "    # backward\n",
    "    dYhat = (2.0 / N) * (y_hat - y)     # (N,O)\n",
    "    dW = X.T @ dYhat                     # (D,O)\n",
    "    db = dYhat.sum(axis=0)               # (O,)\n",
    "\n",
    "    # update\n",
    "    W -= lr * dW\n",
    "    b -= lr * db\n",
    "\n",
    "print(\"W:\", W.ravel(), \"b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8e906-2a56-413d-91c8-8e5b7f23cc93",
   "metadata": {},
   "source": [
    "## 2) Two-layer NN (ReLU -> Linear -> Softmax) â€” Backprop\n",
    "\n",
    "**Architecture**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_1 &= X W_1 + \\mathbf{1} b_1^{\\top} \\\\\n",
    "h  &= \\mathrm{ReLU}(Z_1) \\\\\n",
    "\\text{scores} &= h W_2 + \\mathbf{1} b_2^{\\top} \\\\\n",
    "\\text{probs}_{i,j} &= \\frac{\\exp(\\text{scores}_{i,j})}{\\sum_{c=1}^{C} \\exp(\\text{scores}_{i,c})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Cross-entropy loss (labels as class indices \\(y_i\\))**\n",
    "$$\n",
    "L \\;=\\; -\\frac{1}{N}\\sum_{i=1}^{N}\\log\\!\\big(\\text{probs}_{i,\\,y_i}\\big)\n",
    "$$\n",
    "\n",
    "**Softmax + CE gradient (vectorized)**\n",
    "Let $Y \\in \\mathbb{R}^{N \\times C}$ be one-hot labels. Then\n",
    "$$\n",
    "G \\;=\\; \\frac{1}{N}\\,\\big(\\text{probs} - Y\\big)\n",
    "$$\n",
    "\n",
    "**Top linear layer**\n",
    "$$\n",
    "\\nabla_{W_2} L \\;=\\; h^{\\top} G, \n",
    "\\qquad\n",
    "\\nabla_{b_2} L \\;=\\; \\sum_{i=1}^{N} G_{i,\\cdot},\n",
    "\\qquad\n",
    "\\nabla_{h} L \\;=\\; G W_2^{\\top}\n",
    "$$\n",
    "\n",
    "**ReLU backward**\n",
    "Let $Z_1 = X W_1 + \\mathbf{1}\\, b_1^{\\top}$. Then\n",
    "$$\n",
    "\\nabla_{h}^{(\\text{after})} \\;=\\; \\nabla_{h}^{(\\text{before})} \\;\\odot\\; \\mathbb{I}[\\,Z_1 > 0\\,]\n",
    "$$\n",
    "\n",
    "**Bottom linear layer**\n",
    "$$\n",
    "\\nabla_{W_1} L \\;=\\; X^{\\top}\\,\\nabla_{h}^{(\\text{after})},\n",
    "\\qquad\n",
    "\\nabla_{b_1} L \\;=\\; \\sum_{i=1}^{N} \\nabla_{h,i}^{(\\text{after})}\n",
    "$$\n",
    "\n",
    "**Shape reference**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&X\\in\\mathbb{R}^{N\\times D},\\quad W_1\\in\\mathbb{R}^{D\\times H},\\quad b_1\\in\\mathbb{R}^{H} \\\\\n",
    "&h\\in\\mathbb{R}^{N\\times H},\\quad W_2\\in\\mathbb{R}^{H\\times C},\\quad b_2\\in\\mathbb{R}^{C} \\\\\n",
    "&\\text{scores},\\,\\text{probs},\\,Y,\\,G\\in\\mathbb{R}^{N\\times C}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c367d-553f-4406-b773-a2ea5f1c8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "N, D, H, C = 200, 4, 16, 3        # samples, input dim, hidden, classes\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(0, C, size=N)\n",
    "\n",
    "W1 = 0.01*np.random.randn(D, H); b1 = np.zeros(H)\n",
    "W2 = 0.01*np.random.randn(H, C); b2 = np.zeros(C)\n",
    "\n",
    "lr = 1e-0\n",
    "\n",
    "for _ in range(1000):\n",
    "    # ----- forward -----\n",
    "    z1 = X @ W1 + b1           # (N,H)\n",
    "    h = np.maximum(0, z1)      # ReLU (N,H)\n",
    "    scores = h @ W2 + b2       # (N,C)\n",
    "\n",
    "    # softmax (stable)\n",
    "    scores_shift = scores - scores.max(axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(scores_shift)\n",
    "    probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)   # (N,C)\n",
    "\n",
    "    # cross-entropy loss\n",
    "    correct_logprobs = -np.log(probs[np.arange(N), y] + 1e-12)\n",
    "    ## Binary case\n",
    "    # loss = -np.mean(y * np.log(probs + 1e-12) + (1 - y) * np.log(1 - probs + 1e-12))\n",
    "    loss = correct_logprobs.mean()\n",
    "\n",
    "    # ----- backward -----\n",
    "    dscores = probs.copy()                    # (N,C)\n",
    "    dscores[np.arange(N), y] -= 1\n",
    "    dscores /= N\n",
    "\n",
    "    dW2 = h.T @ dscores                      # (H,C)\n",
    "    db2 = dscores.sum(axis=0)                # (C,)\n",
    "\n",
    "    dh = dscores @ W2.T                      # (N,H)\n",
    "    dh[z1 <= 0] = 0                          # ReLU backprop\n",
    "\n",
    "    dW1 = X.T @ dh                           # (D,H)\n",
    "    db1 = dh.sum(axis=0)                     # (H,)\n",
    "\n",
    "    # ----- update -----\n",
    "    W1 -= lr * dW1; b1 -= lr * db1\n",
    "    W2 -= lr * dW2; b2 -= lr * db2\n",
    "\n",
    "# quick sanity check: accuracy\n",
    "scores = np.maximum(0, X @ W1 + b1) @ W2 + b2\n",
    "pred = scores.argmax(axis=1)\n",
    "acc = (pred == y).mean()\n",
    "print(f\"loss={loss:.4f}, acc={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1828653-ae6b-4ed4-9d13-9fca3db77173",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d38d7f-19dd-464e-9ec0-c2882051bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_heads: int):\n",
    "        ## Identical to super().__init__\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size must be divisible by the number of heads.\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values: torch.Tensor, keys: torch.Tensor, queries: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Split the embedding into self.num_heads different pieces\n",
    "        values = self.values(values).reshape(batch_size, value_len, self.num_heads, self.head_dim)\n",
    "        keys = self.keys(keys).reshape(batch_size, key_len, self.num_heads, self.head_dim)\n",
    "        queries = self.queries(queries).reshape(batch_size, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose for attention dot product: (batch_size, num_heads, seq_len, head_dim)\n",
    "        values = values.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        ## Before queries = [B, num_heads, query_len, head_dim]\n",
    "        ##        keys    = [B, num_heads, key_len, head_dim]\n",
    "        ## keys.transpose(-2, -1)) swaps the last 2 dimensions ==> (B, H, N, L)\n",
    "        energy = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        ## Mask usually has shape [batch_size, key_len]\n",
    "        ## Unsqueeze make it [B, 1, 1, K_len]\n",
    "        if mask is not None:\n",
    "            # Expand mask to match energy shape\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            mask = mask.expand(-1, self.num_heads, query_len, -1)\n",
    "            energy = energy.masked_fill(mask == 0, -float(\"Inf\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)  # (batch_size, num_heads, query_len, head_dim)\n",
    "        ## forces PyTorch to actually copy the tensor into a new block of memory laid out in row-major (C-style) order.\n",
    "        out = out.transpose(1, 2).contiguous()  # (batch_size, query_len, num_heads, head_dim)\n",
    "        out = out.reshape(batch_size, query_len, self.embed_size)  # Concatenate heads\n",
    "\n",
    "        out = self.fc_out(out)  # Apply the final linear layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3009596-2523-4ce1-84a3-e2b8ce750f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_heads: int):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size must be divisible by the number of heads.\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        # 5000 is the max relative distance\n",
    "        self.relative_position_embeddings = nn.Parameter(\n",
    "            torch.randn((2 * 5000 - 1, self.head_dim))\n",
    "        )\n",
    "\n",
    "    def forward(self, values: torch.Tensor, keys: torch.Tensor, queries: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        values = self.values(values).reshape(batch_size, value_len, self.num_heads, self.head_dim)\n",
    "        keys = self.keys(keys).reshape(batch_size, key_len, self.num_heads, self.head_dim)\n",
    "        queries = self.queries(queries).reshape(batch_size, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values = values.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        ## Trick to use broadcast\n",
    "        relative_positions = torch.arange(query_len).unsqueeze(1) - torch.arange(key_len).unsqueeze(0)\n",
    "        # Example:\n",
    "        # tensor([[2, 1, 0, 0],\n",
    "        #         [3, 2, 1, 0],\n",
    "        #         [4, 3, 2, 1],\n",
    "        #         [4, 4, 3, 2]])\n",
    "        clipped_positions = torch.clamp(relative_positions + 5000 - 1, min=0, max=2 * 5000 - 2)\n",
    "        ## EâˆˆR(2Lâˆ’1)Ã—dh look up table\n",
    "        relative_embeddings = self.relative_position_embeddings[clipped_positions]  # (query_len, key_len, head_dim)\n",
    "\n",
    "        # Scaled dot-product attention with relative positional embeddings\n",
    "        energy = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5\n",
    "        # for each position q, we extract k of d dimensional position verctor\n",
    "        # You give it a string equation that describes how indices line up, and PyTorch does the math automatically.\n",
    "        energy += torch.einsum('bhqd,qkd->bhqk', queries, relative_embeddings)  # Add relative positional embeddings\n",
    "\n",
    "        if mask is not None:\n",
    "            # Expand mask to match energy shape\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            mask = mask.expand(-1, self.num_heads, query_len, -1)\n",
    "            energy = energy.masked_fill(mask == 0, -float(\"Inf\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.reshape(batch_size, query_len, self.embed_size)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73289f2-670a-4ba2-b3c0-c216ca99ed48",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4cb9b-e0a2-4343-b3fd-77c3533ffcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"hello ðŸ˜Š\" â†’ \"ðŸ˜Š\" becomes <unk>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380ba47-577b-4612-a541-58269935e20b",
   "metadata": {},
   "source": [
    "The tokenizer only knows merge rules learned during training.\n",
    "If a new word contains a character sequence that never appeared during training,\n",
    "the tokenizer doesnâ€™t know how to split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0f1ed-5339-4918-b878-b08eda47385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, vocab_size: int, min_frequency: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer with the desired vocabulary size and minimum frequency for pair merges.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def train(self, corpus: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on the given corpus.\n",
    "        \"\"\"\n",
    "        # Build initial word frequency and split words into characters\n",
    "        word_freqs = defaultdict(int)\n",
    "        for text in corpus:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                word = word + ' '  # Add a space at the end of each word to preserve word boundaries\n",
    "                word_freqs[word] += 1\n",
    "        ## word_freqs = {\"the \": 2, \"cat \": 1, \"dog \": 1}\n",
    "\n",
    "        # Initialize splits: each word is split into a list of characters\n",
    "        splits = {word: list(word) for word in word_freqs}\n",
    "\n",
    "        # splits = {\n",
    "        #   \"the \": [\"t\", \"h\", \"e\", \" \"],\n",
    "        #   \"cat \": [\"c\", \"a\", \"t\", \" \"],\n",
    "        #   \"dog \": [\"d\", \"o\", \"g\", \" \"]\n",
    "        # }\n",
    "\n",
    "        # Build vocabulary with single characters\n",
    "        alphabet = set()\n",
    "        for word in splits:\n",
    "            for char in splits[word]:\n",
    "                alphabet.add(char)\n",
    "\n",
    "        ## alphabet = {\"t\", \"h\", \"e\", \" \", \"c\", \"a\", \"d\", \"o\", \"g\"}\n",
    "\n",
    "        # Initialize vocabulary with single characters\n",
    "        self.vocab = {char: idx for idx, char in enumerate(alphabet)}\n",
    "\n",
    "        # self.vocab = {\n",
    "        #   \"t\": 0,\n",
    "        #   \"h\": 1,\n",
    "        #   \"e\": 2,\n",
    "        #   \" \": 3,\n",
    "        #   \"c\": 4,\n",
    "        #   \"a\": 5,\n",
    "        #   \"d\": 6,\n",
    "        #   \"o\": 7,\n",
    "        #   \"g\": 8\n",
    "        # }\n",
    "\n",
    "        # Continue merging pairs until the vocabulary reaches the desired size\n",
    "        # After merging \"t\" and \"h\", we have \"t\", \"h\" and \"th\"\n",
    "        # {\n",
    "        #   ('t', 'h'): 2,\n",
    "        #   ('h', 'e'): 2,\n",
    "        #   ('e', ' '): 2,\n",
    "        #   ('c', 'a'): 1,\n",
    "        #   ('a', 't'): 1,\n",
    "        #   ('t', ' '): 1,\n",
    "        #   ('d', 'o'): 1,\n",
    "        #   ('o', 'g'): 1,\n",
    "        #   ('g', ' '): 1\n",
    "        # }\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freqs = self._get_pair_frequencies(splits, word_freqs)\n",
    "\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "\n",
    "            # Get the most frequent pair of symbols\n",
    "            best_pair, best_freq = max(pair_freqs.items(), key=lambda x: x[1])\n",
    "\n",
    "            if best_freq < self.min_frequency:\n",
    "                break\n",
    "\n",
    "            # Merge the most frequent pair\n",
    "            splits = self._merge_pair(best_pair, splits)\n",
    "\n",
    "            # splits = {\n",
    "            #   \"the \": [\"th\", \"e\", \" \"],\n",
    "            #   \"cat \": [\"c\", \"a\", \"t\", \" \"],\n",
    "            #   \"dog \": [\"d\", \"o\", \"g\", \" \"]\n",
    "            # }\n",
    "            \n",
    "            # Add the new merge to the vocabulary and merges table\n",
    "            new_symbol = best_pair[0] + best_pair[1]\n",
    "            ## \"th\"\n",
    "            self.vocab[new_symbol] = len(self.vocab)\n",
    "            self.merges[best_pair] = new_symbol\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token ids.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "\n",
    "        for word in words:\n",
    "            word = word + ' '  # Add a space at the end of each word to preserve word boundaries\n",
    "            split_word = list(word)\n",
    "            for pair, merge in self.merges.items():\n",
    "                i = 0\n",
    "                while i < len(split_word) - 1:\n",
    "                    if split_word[i] == pair[0] and split_word[i + 1] == pair[1]:\n",
    "                        split_word = split_word[:i] + [merge] + split_word[i + 2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            # Convert merged word parts to token ids\n",
    "            for token in split_word:\n",
    "                tokens.append(self.vocab.get(token, -1))  # -1 for unknown tokens\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode the list of token ids back into text.\n",
    "        \"\"\"\n",
    "        ## tokens = [11, 4, 5, 0, 3]\n",
    "        ## [\"the \", \"c\", \"a\", \"t\", \" \"]\n",
    "        # Inverse the vocabulary dictionary to get symbol to token mapping\n",
    "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        decoded_text = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in inv_vocab:\n",
    "                decoded_text.append(inv_vocab[token])\n",
    "            else:\n",
    "                decoded_text.append('<UNK>')  # Handle unknown tokens\n",
    "\n",
    "        # Join the decoded text and strip the trailing spaces after joining\n",
    "        return ''.join(decoded_text).strip()\n",
    "\n",
    "    def _get_pair_frequencies(self, splits: dict, word_freqs: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Get the frequency of all symbol pairs in the current split words.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in word_freqs.items():\n",
    "            split = splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "    def _merge_pair(self, pair: Tuple[str, str], splits: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Merge the given pair in all the words.\n",
    "        \"\"\"\n",
    "        for word in splits:\n",
    "            split = splits[word]\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [pair[0] + pair[1]] + split[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[word] = split\n",
    "        return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e2020-fd1e-4454-8ea3-4a148dc4bc76",
   "metadata": {},
   "source": [
    "## Beams Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd2629-0105-4f2b-892a-e7474c9e07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "EOS_TOKEN = 2\n",
    "VOCAB = [\"hello\", \"world\", \"how\", \"are\", \"you\", \"doing\", \"today\", \"goodbye\", \"thanks\", \"please\"]\n",
    "\n",
    "class DummyModel:\n",
    "    def next_token_proba(self, sequence):\n",
    "        vocab_size = len(VOCAB)\n",
    "        return torch.softmax(torch.rand(vocab_size), dim=0)\n",
    "\n",
    "def beam_search(model, start_token, k, max_length):\n",
    "    \"\"\"\n",
    "    Perform beam search to generate the most probable sequence.\n",
    "\n",
    "    Args:\n",
    "        model: An object with a method next_token_proba(sequence) -> torch.Tensor.\n",
    "        start_token (int): The index of the start token in the vocabulary.\n",
    "        k (int): Beam width, the number of sequences to keep at each step.\n",
    "        max_length (int): The maximum length of the sequence to generate.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: The most probable sequence of token indices.\n",
    "    \"\"\"\n",
    "    # Initialize the beam with the start token and a score of 0.0\n",
    "    sequences = [(0.0, [start_token])]  # Each item is a tuple: (score, sequence)\n",
    "\n",
    "    epsilon = 1e-10  # Small constant to prevent log(0)\n",
    "\n",
    "    # Iterate up to the maximum sequence length\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []  # List to store all potential sequences at this step\n",
    "        completed = True     # Flag to check if all sequences have ended\n",
    "\n",
    "        # Loop over all sequences in the current beam\n",
    "        for score, seq in sequences:\n",
    "            # If the last token is EOS_TOKEN or max_length is reached, keep the sequence as is\n",
    "            if seq[-1] == EOS_TOKEN or len(seq) >= max_length:\n",
    "                all_candidates.append((score, seq))\n",
    "                continue  # Skip to the next sequence\n",
    "\n",
    "            completed = False  # At least one sequence can still be expanded\n",
    "\n",
    "            # Get the probabilities of the next tokens from the model\n",
    "            probs = model.next_token_proba(seq)\n",
    "\n",
    "            # Select the top k token probabilities and their corresponding indices\n",
    "            top_k = min(k, len(probs))  # Adjust k if it's larger than the vocabulary size\n",
    "            top_k_probs, top_k_tokens = torch.topk(probs, top_k)\n",
    "\n",
    "            # top_k_probs = tensor([0.4, 0.25, 0.2])\n",
    "            # top_k_tokens = tensor([1, 4, 2])\n",
    "            # Create new candidate sequences by appending each of the top k tokens\n",
    "            for i in range(len(top_k_tokens)):\n",
    "                token = top_k_tokens[i].item()       # Index of the next token\n",
    "                prob = top_k_probs[i].item()         # Probability of the next token\n",
    "                prob = max(prob, epsilon)            # Ensure probability is not zero\n",
    "\n",
    "                # Calculate the new score by adding the negative log probability\n",
    "                candidate_score = score - torch.log(torch.tensor(prob))\n",
    "\n",
    "                # Create a new sequence by appending the token to the existing sequence\n",
    "                candidate_seq = seq + [token]\n",
    "\n",
    "                # Add the new candidate sequence and its score to the list of all candidates\n",
    "                all_candidates.append((candidate_score.item(), candidate_seq))\n",
    "\n",
    "        # If all sequences have ended, exit the loop early to save computation\n",
    "        if completed:\n",
    "            break\n",
    "\n",
    "        # Sort all candidate sequences by their scores in ascending order (lower score is better)\n",
    "        all_candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Select the top k sequences with the best scores to form the new beam\n",
    "        sequences = all_candidates[:k]\n",
    "\n",
    "    # Return the sequence with the best score (first in the sorted list)\n",
    "    return sequences[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1314654-2852-48a2-b832-43de4c83b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_with_length_norm(model, start_token, k, max_length, alpha):\n",
    "    \"\"\"\n",
    "    Perform beam search with length normalization to generate the most probable sequence.\n",
    "\n",
    "    Args:\n",
    "        model: An object with a method next_token_proba(sequence) -> torch.Tensor.\n",
    "        start_token (int): The index of the start token in the vocabulary.\n",
    "        k (int): Beam width, the number of sequences to keep at each step.\n",
    "        max_length (int): The maximum length of the sequence to generate.\n",
    "        alpha (float): Length normalization parameter (0 < alpha <= 1).\n",
    "\n",
    "    Returns:\n",
    "        List[int]: The most probable sequence of token indices.\n",
    "    \"\"\"\n",
    "    # Initialize the beam with the start token and a score of 0.0\n",
    "    sequences = [(0.0, [start_token])]  # Each item is a tuple: (score, sequence)\n",
    "\n",
    "    epsilon = 1e-10  # Small constant to prevent log(0)\n",
    "\n",
    "    # Iterate up to the maximum sequence length\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []  # List to store all potential sequences at this step\n",
    "        completed = True     # Flag to check if all sequences have ended\n",
    "\n",
    "        # Loop over all sequences in the current beam\n",
    "        for score, seq in sequences:\n",
    "            # If the last token is EOS_TOKEN or max_length is reached, keep the sequence as is\n",
    "            if seq[-1] == EOS_TOKEN or len(seq) >= max_length:\n",
    "                normalized_score = score / (len(seq) ** alpha)\n",
    "                all_candidates.append((normalized_score, seq))\n",
    "                continue  # Skip to the next sequence\n",
    "\n",
    "            completed = False  # At least one sequence can still be expanded\n",
    "\n",
    "            # Get the probabilities of the next tokens from the model\n",
    "            probs = model.next_token_proba(seq)\n",
    "\n",
    "            # Select the top k token probabilities and their corresponding indices\n",
    "            top_k = min(k, len(probs))  # Adjust k if it's larger than the vocabulary size\n",
    "            top_k_probs, top_k_tokens = torch.topk(probs, top_k)\n",
    "\n",
    "            # Create new candidate sequences by appending each of the top k tokens\n",
    "            for i in range(len(top_k_tokens)):\n",
    "                token = top_k_tokens[i].item()       # Index of the next token\n",
    "                prob = top_k_probs[i].item()         # Probability of the next token\n",
    "                prob = max(prob, epsilon)            # Ensure probability is not zero\n",
    "\n",
    "                # Calculate the new score by adding the negative log probability\n",
    "                candidate_score = score - torch.log(torch.tensor(prob))\n",
    "\n",
    "                # Create a new sequence by appending the token to the existing sequence\n",
    "                candidate_seq = seq + [token]\n",
    "\n",
    "                # Add the new candidate sequence and its score to the list of all candidates\n",
    "                all_candidates.append((candidate_score.item(), candidate_seq))\n",
    "\n",
    "        # If all sequences have ended, exit the loop early to save computation\n",
    "        if completed:\n",
    "            break\n",
    "\n",
    "        # Sort all candidate sequences by their scores in ascending order (lower score is better)\n",
    "        all_candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Select the top k sequences with the best scores to form the new beam\n",
    "        sequences = all_candidates[:k]\n",
    "\n",
    "    # Return the sequence with the best normalized score (first in the sorted list)\n",
    "    return sequences[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b706d-7843-4546-a7a8-22f7f03ac273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_with_length_norm_and_diversity(model, start_token, k, max_length, alpha, diversity_groups, top_p):\n",
    "    \"\"\"\n",
    "    Perform beam search with length normalization, diverse beam search, and top-p sampling to generate sequences.\n",
    "\n",
    "    Args:\n",
    "        model: An object with a method next_token_proba(sequence) -> torch.Tensor.\n",
    "        start_token (int): The index of the start token in the vocabulary.\n",
    "        k (int): Beam width, the number of sequences to keep at each step.\n",
    "        max_length (int): The maximum length of the sequence to generate.\n",
    "        alpha (float): Length normalization parameter (0 < alpha <= 1).\n",
    "        diversity_groups (int): Number of diverse groups for Diverse Beam Search.\n",
    "        top_p (float): Cumulative probability threshold for Top-p (nucleus) sampling.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: The most probable sequence of token indices.\n",
    "    \"\"\"\n",
    "    # Divide the beam width across diversity groups\n",
    "    group_width = max(1, k // diversity_groups)\n",
    "    sequences = [[(0.0, [start_token])] for _ in range(diversity_groups)]  # List of beams for each group\n",
    "\n",
    "    epsilon = 1e-10  # Small constant to prevent log(0)\n",
    "\n",
    "    # Iterate up to the maximum sequence length\n",
    "    for _ in range(max_length):\n",
    "        all_group_candidates = []\n",
    "        completed = True\n",
    "\n",
    "        for group_id in range(diversity_groups):\n",
    "            group_sequences = sequences[group_id]\n",
    "            all_candidates = []\n",
    "\n",
    "            # Loop over all sequences in the current group's beam\n",
    "            for score, seq in group_sequences:\n",
    "                # If the last token is EOS_TOKEN or max_length is reached, keep the sequence as is\n",
    "                if seq[-1] == EOS_TOKEN or len(seq) >= max_length:\n",
    "                    normalized_score = score / (len(seq) ** alpha)\n",
    "                    all_candidates.append((normalized_score, seq))\n",
    "                    continue  # Skip to the next sequence\n",
    "\n",
    "                completed = False\n",
    "\n",
    "                # Get the probabilities of the next tokens from the model\n",
    "                probs = model.next_token_proba(seq)\n",
    "\n",
    "                # Apply top-p (nucleus) sampling to filter tokens\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "                top_p_mask = cumulative_probs <= top_p\n",
    "                # top_p_mask = tensor([True, True, True, False, False])\n",
    "                top_p_indices = sorted_indices[top_p_mask]\n",
    "                top_p_probs = sorted_probs[top_p_mask]\n",
    "                # top_p_indices = tensor([2, 1, 3])\n",
    "                # top_p_probs = tensor([0.40, 0.20, 0.15])\n",
    "\n",
    "                # If no tokens are below the threshold, take at least one\n",
    "                if len(top_p_indices) == 0:\n",
    "                    top_p_indices = sorted_indices[:1]\n",
    "                    top_p_probs = sorted_probs[:1]\n",
    "\n",
    "                # Create new candidate sequences by appending each of the top-p tokens\n",
    "                for i in range(len(top_p_indices)):\n",
    "                    token = top_p_indices[i].item()  # Index of the next token\n",
    "                    prob = top_p_probs[i].item()  # Probability of the next token\n",
    "                    prob = max(prob, epsilon)  # Ensure probability is not zero\n",
    "\n",
    "                    # Calculate the new score by adding the negative log probability\n",
    "                    candidate_score = score - torch.log(torch.tensor(prob))\n",
    "\n",
    "                    # Create a new sequence by appending the token to the existing sequence\n",
    "                    candidate_seq = seq + [token]\n",
    "\n",
    "                    # Add the new candidate sequence and its score to the list of all candidates\n",
    "                    all_candidates.append((candidate_score.item(), candidate_seq))\n",
    "\n",
    "            # Sort all candidate sequences by their scores in ascending order (lower score is better)\n",
    "            all_candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "            # Select the top sequences for this group\n",
    "            sequences[group_id] = all_candidates[:group_width]\n",
    "\n",
    "            # Collect candidates for all groups\n",
    "            all_group_candidates.extend(all_candidates[:group_width])\n",
    "\n",
    "        # If all sequences have ended, exit the loop early to save computation\n",
    "        if completed:\n",
    "            break\n",
    "\n",
    "    # Flatten and sort all group candidates to get the best sequences\n",
    "    all_group_candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Return the top sequence from all groups\n",
    "    return all_group_candidates[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30b037-bae3-485e-a8fd-012cd8d29b37",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff85dbb-c1f5-40e7-b420-c2a5dbb60cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431542a-8fd7-4a3c-bd0d-2c2de57a601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.encoder_attention = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Masked Multi-head self-attention\n",
    "        attn_output = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Multi-head attention over encoder output\n",
    "        attn_output = self.encoder_attention(enc_output, enc_output, x, src_mask)\n",
    "        x = self.layer_norm2(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(num_encoder_layers, d_model, num_heads, d_ff, dropout_rate)\n",
    "        self.decoder = TransformerDecoder(num_decoder_layers, d_model, num_heads, d_ff, dropout_rate)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        out = self.fc_out(dec_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe9ac2-d5b0-4f73-b781-56a9c3853248",
   "metadata": {},
   "source": [
    "## GD\n",
    "\n",
    "We want to choose $\\Delta \\theta$ that minimizes this local approximation:\n",
    "\n",
    "$$\n",
    "\\min_{\\Delta \\theta} \\; L(\\theta_t) + \\nabla L(\\theta_t)^\\top \\Delta \\theta\n",
    "$$\n",
    "\n",
    "Since $L(\\theta_t)$ is constant with respect to $\\Delta \\theta$, we only care about minimizing:\n",
    "\n",
    "$$\n",
    "\\nabla L(\\theta_t)^\\top \\Delta \\theta\n",
    "$$\n",
    "\n",
    "This is a dot product. To make it as **negative as possible**, $\\Delta \\theta$ should point in the **opposite direction** of the gradient:\n",
    "\n",
    "$$\n",
    "\\Delta \\theta = -\\eta \\, \\nabla L(\\theta_t)\n",
    "$$\n",
    "\n",
    "(where $\\eta > 0$ controls the step size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca0f44-9b7a-4be9-9fab-5b0dfe92572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    # Define the function f(x) = (x - 3)^2\n",
    "    def f(x):\n",
    "        return (x - 3) ** 2\n",
    "    \n",
    "    # Define the derivative of the function f'(x) = 2 * (x - 3)\n",
    "    def gradient(x):\n",
    "        return 2 * (x - 3)\n",
    "    \n",
    "    # Initialize x with the initial guess\n",
    "    x = initial_x\n",
    "    \n",
    "    # Perform SGD for the specified number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        # Update x using the gradient and learning rate\n",
    "        x = x - learning_rate * gradient(x)    \n",
    "        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d38fcd-06b0-4525-952d-c62835e40859",
   "metadata": {},
   "source": [
    "## ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fe64a-a22f-4fd7-915f-e443f924918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def compute_online_elo(battles, K=4, SCALE=400, BASE=10, INIT_RATING=1000):\n",
    "    # Default rating initialization for each model\n",
    "    rating = defaultdict(lambda: INIT_RATING)\n",
    "    \n",
    "    # Iterate through the dataframe to update the Elo ratings\n",
    "    for rd, model_a, model_b, winner in battles[['model_a', 'model_b', 'winner']].itertuples():\n",
    "        ra = rating[model_a]\n",
    "        rb = rating[model_b]\n",
    "        \n",
    "        # Calculate expected outcomes for both models\n",
    "        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n",
    "        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n",
    "        \n",
    "        # Determine the actual score for model_a based on the winner\n",
    "        if winner == \"model_a\":\n",
    "            sa = 1  # model_a wins\n",
    "        elif winner == \"model_b\":\n",
    "            sa = 0  # model_b wins\n",
    "        elif winner == \"tie\":\n",
    "            sa = 0.5  # Tie\n",
    "        else:\n",
    "            raise Exception(f\"unexpected vote {winner}\")\n",
    "        \n",
    "        # Update ratings based on the outcome\n",
    "        rating[model_a] += K * (sa - ea)\n",
    "        rating[model_b] += K * ((1 - sa) - eb)\n",
    "    \n",
    "    return rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c96b7-2fac-4219-b9d2-8b457ad774da",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Valdation\n",
    "\n",
    "Stratified k-fold is similar to k-fold cross-validation, but with an important difference: it ensures that each fold maintains the same class distribution as the original dataset. This is especially useful for imbalanced classification problems, where some classes may have very few examples. By maintaining the proportion of classes across all folds, stratified k-fold provides a more representative and consistent evaluation of the model's performance than standard k-fold cross-validation. But it could be slightly more complex to implement, especially for multiclass problems.\n",
    "\n",
    "In leave-one-out cross-validation, each data point in the dataset is treated as a test set one at a time. This means that if there are n samples, the model will be trained n times, each time using n-1 samples for training and the remaining sample for testing. Leave-one-out cross-validation makes full use of the dataset and provides an almost unbiased estimate of model performance and is particularly useful for small datasets. But obviously, it is more computationally expensive for large datasets since it requires training the model as many times as there are data points. It also tends to have high variance, as the performance may vary a lot for different single observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c55ab-ead8-4f62-ba77-d1ebc38a8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def custom_stratified_k_fold_cross_validation(model, X, y, k=5):\n",
    "    \"\"\"\n",
    "    Function to implement stratified k-fold cross-validation manually.\n",
    "    \n",
    "    Parameters:\n",
    "    model : sklearn model\n",
    "        The model to be trained and evaluated.\n",
    "    X : ndarray\n",
    "        Features dataset.\n",
    "    y : ndarray\n",
    "        Target labels.\n",
    "    k : int\n",
    "        Number of folds for cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "    float\n",
    "        Average test error across all folds.\n",
    "    \"\"\"\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Step 1: Split the data into k stratified folds based on the labels\n",
    "    # if unique_classes=[0,2] and y=[2,0,2], then class_indices=[1,0,1]\n",
    "    unique_classes, class_indices = np.unique(y, return_inverse=True)\n",
    "\n",
    "    # Counts how many samples belong to each class. Used to stratify.\n",
    "    class_counts = np.bincount(class_indices)\n",
    "    \n",
    "    # Create stratified folds by allocating samples from each class into folds\n",
    "    stratified_indices = [[] for _ in range(k)]\n",
    "    for class_idx, count in enumerate(class_counts):\n",
    "        class_sample_indices = np.where(class_indices == class_idx)[0]\n",
    "        np.random.shuffle(class_sample_indices)\n",
    "        # Distribute these samples across folds\n",
    "        for i, sample_index in enumerate(class_sample_indices):\n",
    "            stratified_indices[i % k].append(sample_index)\n",
    "    \n",
    "    # Convert list of lists to list of numpy arrays for easier indexing\n",
    "    stratified_indices = [np.array(fold) for fold in stratified_indices]\n",
    "\n",
    "    # Step 2: Perform cross-validation\n",
    "    test_errors = []\n",
    "    for i in range(k):\n",
    "        # Get test indices and training indices\n",
    "        test_indices = stratified_indices[i]\n",
    "        train_indices = np.hstack([stratified_indices[j] for j in range(k) if j != i])\n",
    "        \n",
    "        # Split into training and testing data\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate mean squared error for this fold\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        test_errors.append(mse)\n",
    "    \n",
    "    # Step 3: Calculate the average test error across all folds\n",
    "    average_test_error = np.mean(test_errors)\n",
    "    \n",
    "    return average_test_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f68f4f-f584-462b-a37e-66bc42ec19c3",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324b275-58cf-45fa-bdda-775c1f8b6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d, 1)  # includes bias\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x)).squeeze(-1)\n",
    "\n",
    "model = LR(d=X.shape[1])\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.BCELoss()  # or BCEWithLogitsLoss with raw scores\n",
    "\n",
    "X_t = torch.tensor(X, dtype=torch.float32)\n",
    "y_t = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "for _ in range(1000):\n",
    "    opt.zero_grad()\n",
    "    p = model(X_t)\n",
    "    loss = loss_fn(p, y_t)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc5e78-dd71-4bd7-bc7e-367438faba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, lr=0.1, n_iters=1000, reg=0.0, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.reg = reg\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        # numerically stable sigmoid\n",
    "        pos = z >= 0\n",
    "        neg = ~pos\n",
    "        out = np.empty_like(z, dtype=float)\n",
    "        out[pos] = 1 / (1 + np.exp(-z[pos]))\n",
    "        expz = np.exp(z[neg])\n",
    "        out[neg] = expz / (1 + expz)\n",
    "        return out\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float).reshape(-1)\n",
    "        n, d = X.shape\n",
    "        self.w = np.zeros(d)\n",
    "        self.b = 0.0 if self.fit_intercept else 0.0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            z = X @ self.w + (self.b if self.fit_intercept else 0.0)\n",
    "            y_hat = self._sigmoid(z)\n",
    "\n",
    "            # gradients\n",
    "            err = (y_hat - y) / n\n",
    "            grad_w = X.T @ err + 2 * self.reg * self.w\n",
    "            if self.fit_intercept:\n",
    "                grad_b = err.sum()\n",
    "            # update\n",
    "            self.w -= self.lr * grad_w\n",
    "            if self.fit_intercept:\n",
    "                self.b -= self.lr * grad_b\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        z = X @ self.w + (self.b if self.fit_intercept else 0.0)\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
